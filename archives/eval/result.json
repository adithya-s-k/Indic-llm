[{"results": {"arc_challenge": {"acc,null": 0.44283276450511944, "acc_stderr,null": 0.014515573873348906, "acc_norm,null": 0.4735494880546075, "acc_norm_stderr,null": 0.014590931358120165, "alias": "arc_challenge"}}, "configs": {"arc_challenge": {"task": "arc_challenge", "group": ["ai2_arc"], "dataset_path": "allenai/ai2_arc", "dataset_name": "ARC-Challenge", "training_split": "train", "validation_split": "validation", "test_split": "test", "doc_to_text": "Question: {{question}}\nAnswer:", "doc_to_target": "{{choices.label.index(answerKey)}}", "doc_to_choice": "{{choices.text}}", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 25, "metric_list": [{"metric": "acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "acc_norm", "aggregation": "mean", "higher_is_better": True}], "output_type": "multiple_choice", "repeats": 1, "should_decontaminate": True, "doc_to_decontamination_query": "Question: {{question}}\nAnswer:", "metadata": {"version": 1.0}}}, "versions": {"arc_challenge": 1.0}, "n-shot": {"arc_challenge": 25}, "config": {"model": "vllm", "model_args": "pretrained=Cognitive-Lab/Ambari-7B-base-v0.1,dtype=auto,trust_remote_code=True", "batch_size": "25", "batch_sizes": [], "device": null, "use_cache": null, "limit": null, "bootstrap_iters": 100000, "gen_kwargs": null}, "git_hash": "34cded30"},
{"results": {"hellaswag": {"acc,null": 0.5530770762796255, "acc_stderr,null": 0.004961587574275622, "acc_norm,null": 0.7452698665604461, "acc_norm_stderr,null": 0.004348189459336512, "alias": "hellaswag"}}, "configs": {"hellaswag": {"task": "hellaswag", "group": ["multiple_choice"], "dataset_path": "hellaswag", "training_split": "train", "validation_split": "validation", "process_docs": "<function process_docs at 0x7fe93cc09160>", "doc_to_text": "{{query}}", "doc_to_target": "{{label}}", "doc_to_choice": "choices", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 10, "metric_list": [{"metric": "acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "acc_norm", "aggregation": "mean", "higher_is_better": True}], "output_type": "multiple_choice", "repeats": 1, "should_decontaminate": False, "metadata": {"version": 1.0}}}, "versions": {"hellaswag": 1.0}, "n-shot": {"hellaswag": 10}, "config": {"model": "vllm", "model_args": "pretrained=Cognitive-Lab/Ambari-7B-base-v0.1,dtype=auto,trust_remote_code=True", "batch_size": "auto", "batch_sizes": [], "device": null, "use_cache": null, "limit": null, "bootstrap_iters": 100000, "gen_kwargs": null}, "git_hash": "34cded30"},
{"results": {"truthfulqa": {"acc,null": 0.28810763057572925, "acc_stderr,null": 0.05103124927256592, "bleu_max,null": 26.580385658437926, "bleu_max_stderr,null": 0.6301946748898342, "bleu_acc,null": 0.3574051407588739, "bleu_acc_stderr,null": 0.00028145429671323774, "bleu_diff,null": -5.869264038375872, "bleu_diff_stderr,null": 0.7942662807101265, "rouge1_max,null": 51.69083333623398, "rouge1_max_stderr,null": 0.7809950678610036, "rouge1_acc,null": 0.3292533659730722, "rouge1_acc_stderr,null": 0.00027064410167705296, "rouge1_diff,null": -6.86209820747026, "rouge1_diff_stderr,null": 1.0672287717442837, "rouge2_max,null": 35.57040537733373, "rouge2_max_stderr,null": 1.0762998265480708, "rouge2_acc,null": 0.2766217870257038, "rouge2_acc_stderr,null": 0.0002452232524122683, "rouge2_diff,null": -8.98052016069942, "rouge2_diff_stderr,null": 1.3905531878760973, "rougeL_max,null": 48.66067219209134, "rougeL_max_stderr,null": 0.8046448649702134, "rougeL_acc,null": 0.3182374541003672, "rougeL_acc_stderr,null": 0.000265885265818729, "rougeL_diff,null": -7.33559007672252, "rougeL_diff_stderr,null": 1.0790760967114559, "alias": "truthfulqa"}, "truthfulqa_gen": {"bleu_max,null": 26.580385658437926, "bleu_max_stderr,null": 0.7938480175007268, "bleu_acc,null": 0.3574051407588739, "bleu_acc_stderr,null": 0.016776599676729422, "bleu_diff,null": -5.869264038375872, "bleu_diff_stderr,null": 0.8912161806824013, "rouge1_max,null": 51.69083333623398, "rouge1_max_stderr,null": 0.8837392533213649, "rouge1_acc,null": 0.3292533659730722, "rouge1_acc_stderr,null": 0.016451264440068215, "rouge1_diff,null": -6.86209820747026, "rouge1_diff_stderr,null": 1.033067651097586, "rouge2_max,null": 35.57040537733373, "rouge2_max_stderr,null": 1.0374487103216576, "rouge2_acc,null": 0.2766217870257038, "rouge2_acc_stderr,null": 0.01565960575532693, "rouge2_diff,null": -8.98052016069942, "rouge2_diff_stderr,null": 1.1792171928343385, "rougeL_max,null": 48.66067219209134, "rougeL_max_stderr,null": 0.8970199913994188, "rougeL_acc,null": 0.3182374541003672, "rougeL_acc_stderr,null": 0.016305988648920647, "rougeL_diff,null": -7.33559007672252, "rougeL_diff_stderr,null": 1.0387858762572082, "alias": " - truthfulqa_gen"}, "truthfulqa_mc1": {"acc,null": 0.23623011015911874, "acc_stderr,null": 0.0148697550158711, "alias": " - truthfulqa_mc1"}, "truthfulqa_mc2": {"acc,null": 0.39186267140895026, "acc_stderr,null": 0.013817517989402627, "alias": " - truthfulqa_mc2"}}, "groups": {"truthfulqa": {"acc,null": 0.28810763057572925, "acc_stderr,null": 0.05103124927256592, "bleu_max,null": 26.580385658437926, "bleu_max_stderr,null": 0.6301946748898342, "bleu_acc,null": 0.3574051407588739, "bleu_acc_stderr,null": 0.00028145429671323774, "bleu_diff,null": -5.869264038375872, "bleu_diff_stderr,null": 0.7942662807101265, "rouge1_max,null": 51.69083333623398, "rouge1_max_stderr,null": 0.7809950678610036, "rouge1_acc,null": 0.3292533659730722, "rouge1_acc_stderr,null": 0.00027064410167705296, "rouge1_diff,null": -6.86209820747026, "rouge1_diff_stderr,null": 1.0672287717442837, "rouge2_max,null": 35.57040537733373, "rouge2_max_stderr,null": 1.0762998265480708, "rouge2_acc,null": 0.2766217870257038, "rouge2_acc_stderr,null": 0.0002452232524122683, "rouge2_diff,null": -8.98052016069942, "rouge2_diff_stderr,null": 1.3905531878760973, "rougeL_max,null": 48.66067219209134, "rougeL_max_stderr,null": 0.8046448649702134, "rougeL_acc,null": 0.3182374541003672, "rougeL_acc_stderr,null": 0.000265885265818729, "rougeL_diff,null": -7.33559007672252, "rougeL_diff_stderr,null": 1.0790760967114559, "alias": "truthfulqa"}}, "configs": {"truthfulqa_gen": {"task": "truthfulqa_gen", "group": ["truthfulqa"], "dataset_path": "truthful_qa", "dataset_name": "generation", "validation_split": "validation", "process_docs": "<function process_docs_gen at 0x7f00cff99a60>",  "doc_to_target": " ", "process_results": "<function process_results_gen at 0x7f00cff9f040>", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 0, "metric_list": [{"metric": "bleu_max", "aggregation": "mean", "higher_is_better": True}, {"metric": "bleu_acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "bleu_diff", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge1_max", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge1_acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge1_diff", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge2_max", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge2_acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "rouge2_diff", "aggregation": "mean", "higher_is_better": True}, {"metric": "rougeL_max", "aggregation": "mean", "higher_is_better": True}, {"metric": "rougeL_acc", "aggregation": "mean", "higher_is_better": True}, {"metric": "rougeL_diff", "aggregation": "mean", "higher_is_better": True}], "output_type": "generate_until", "generation_kwargs": {"until": ["\n\n"], "do_sample": False}, "repeats": 1, "should_decontaminate": True, "doc_to_decontamination_query": "question", "metadata": {"version": 3.0}}, "truthfulqa_mc1": {"task": "truthfulqa_mc1", "group": ["truthfulqa"], "dataset_path": "truthful_qa", "dataset_name": "multiple_choice", "validation_split": "validation", "doc_to_text": "{% set prompt_qa = "Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain."%}{{prompt_qa + "\n\nQ: " + question + "\nA:"}}", "doc_to_target": 0, "doc_to_choice": "{{mc1_targets.choices}}", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 0, "metric_list": [{"metric": "acc", "aggregation": "mean", "higher_is_better": True}], "output_type": "multiple_choice", "repeats": 1, "should_decontaminate": True, "doc_to_decontamination_query": "question", "metadata": {"version": 2.0}}, "truthfulqa_mc2": {"task": "truthfulqa_mc2", "group": ["truthfulqa"], "dataset_path": "truthful_qa", "dataset_name": "multiple_choice", "validation_split": "validation", "doc_to_text": "{% set prompt_qa = "Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain."%}{{prompt_qa + "\n\nQ: " + question + "\nA:"}}", "doc_to_target": 0, "doc_to_choice": "{{mc2_targets.choices}}", "process_results": "<function process_results_mc2 at 0x7f00cff9f310>", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 0, "metric_list": [{"metric": "acc", "aggregation": "mean", "higher_is_better": True}], "output_type": "multiple_choice", "repeats": 1, "should_decontaminate": True, "doc_to_decontamination_query": "question", "metadata": {"version": 2.0}}}, "versions": {"truthfulqa": "N/A", "truthfulqa_gen": 3.0, "truthfulqa_mc1": 2.0, "truthfulqa_mc2": 2.0}, "n-shot": {"truthfulqa": 0, "truthfulqa_gen": 0, "truthfulqa_mc1": 0, "truthfulqa_mc2": 0}, "config": {"model": "vllm", "model_args": "pretrained=Cognitive-Lab/Ambari-7B-base-v0.1,dtype=auto,trust_remote_code=True", "batch_size": "auto", "batch_sizes": [], "device": null, "use_cache": null, "limit": null, "bootstrap_iters": 100000, "gen_kwargs": null}, "git_hash": "34cded30"},
{"results": {"winogrande": {"acc,null": 0.7040252565114443, "acc_stderr,null": 0.012829348226339014, "alias": "winogrande"}}, "configs": {"winogrande": {"task": "winogrande", "dataset_path": "winogrande", "dataset_name": "winogrande_xl", "training_split": "train", "validation_split": "validation", "doc_to_text": "<function doc_to_text at 0x7fde6eaf2940>", "doc_to_target": "<function doc_to_target at 0x7fde6eaf2d30>", "doc_to_choice": "<function doc_to_choice at 0x7fde6ea890d0>", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 5, "metric_list": [{"metric": "acc", "aggregation": "mean", "higher_is_better": True}], "output_type": "multiple_choice", "repeats": 1, "should_decontaminate": True, "doc_to_decontamination_query": "sentence", "metadata": {"version": 1.0}}}, "versions": {"winogrande": 1.0}, "n-shot": {"winogrande": 5}, "config": {"model": "vllm", "model_args": "pretrained=Cognitive-Lab/Ambari-7B-base-v0.1,dtype=auto,trust_remote_code=True", "batch_size": "auto", "batch_sizes": [], "device": null, "use_cache": null, "limit": null, "bootstrap_iters": 100000, "gen_kwargs": null}, "git_hash": "34cded30"},
{"results": {"gsm8k": {"exact_match,get-answer": 0.016679302501895376, "exact_match_stderr,get-answer": 0.0035275958887224304, "alias": "gsm8k"}}, "configs": {"gsm8k": {"task": "gsm8k", "group": ["math_word_problems"], "dataset_path": "gsm8k", "dataset_name": "main", "training_split": "train", "test_split": "test", "fewshot_split": "train", "doc_to_text": "Question: {{question}}\nAnswer:", "doc_to_target": "{{answer}}", "description": "", "target_delimiter": " ", "fewshot_delimiter": "\n\n", "num_fewshot": 5, "metric_list": [{"metric": "exact_match", "aggregation": "mean", "higher_is_better": True, "ignore_case": True, "ignore_punctuation": False, "regexes_to_ignore": [",", "\\$", "(?s).*#### "]}], "output_type": "generate_until", "generation_kwargs": {"until": ["\n\n", "Question:"], "do_sample": False}, "repeats": 1, "filter_list": [{"name": "get-answer", "filter": [{"function": "regex", "regex_pattern": "#### (\\-?[0-9\\.\\,]+)"}, {"function": "take_first"}]}], "should_decontaminate": False, "metadata": {"version": 2.0}}}, "versions": {"gsm8k": 2.0}, "n-shot": {"gsm8k": 5}, "config": {"model": "vllm", "model_args": "pretrained=Cognitive-Lab/Ambari-7B-base-v0.1,dtype=auto,trust_remote_code=True", "batch_size": "auto", "batch_sizes": [], "device": null, "use_cache": null, "limit": null, "bootstrap_iters": 100000, "gen_kwargs": null}, "git_hash": "34cded30"}]